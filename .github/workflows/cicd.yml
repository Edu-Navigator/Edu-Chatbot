# cicd.yml

name: Airflow DAG CI/CD

on:
  push:
    branches: [ main, 'feature/**' ]
  pull_request:
    branches: [ main ]

jobs:
  dag-test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Ruff
        run: |
          pip install ruff

      - name: Run Ruff lint
        continue-on-error: true
        run: |
          ruff check .

      - name: Run Ruff format check
        continue-on-error: true
        run: |
          ruff format --check .

      - name: Run Airflow DAG import test
        run: |
          docker compose -f docker-compose.ci.yml up \
            --build \
            --abort-on-container-exit \
            --exit-code-from airflow-test

      - name: Show Docker logs on failure
        if: failure()
        run: |
          docker compose -f docker-compose.ci.yml logs

  # main ë¸Œëœì¹˜ì— pushë  ë•Œë§Œ ë°°í¬
  deploy-to-s3:
    needs: dag-test
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ vars.AWS_REGION }}

    - name: Sync to S3
      run: |
        # DAGsë§Œ ì—…ë¡œë“œ
        aws s3 sync ./airflow/dags/ s3://${{ vars.S3_BUCKET_NAME }}/airflow/dags/ \
          --exclude ".git/*" \
          --exclude ".github/*" \
          --exclude "*.md" \
          --exclude "docker-compose*.yml" \
          --exclude "Dockerfile" \
          --exclude "*.pyc" \
          --exclude "__pycache__/*" \
          --delete
        echo "âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ"
    
  deploy-to-main:
    needs: deploy-to-s3
    runs-on: [self-hosted, airflow-main]
    
    steps:
    - name: Clean pycache
      run: |
        sudo find /home/ubuntu/airflow/dags -type d -name "__pycache__" -exec rm -rf {} +

    - name: Sync DAGs from S3
      run: |
        echo "ğŸš€ Airflow Main ë°°í¬ ì‹œì‘..."
        aws s3 sync s3://${{ vars.S3_BUCKET_NAME }}/airflow/dags/ \
          /home/ubuntu/airflow/dags/ \
          --delete \
          --exclude "__pycache__/*" \
          --exclude "**/__pycache__/*" \
          --exclude "*.pyc"
        echo "âœ… DAGs ë™ê¸°í™” ì™„ë£Œ"

    - name: Restart Airflow (manual)
      run: |
        echo "ğŸ”„ Airflow processes ì¬ì‹œì‘ì¤‘..."

        pkill -f "airflow webserver" || true
        pkill -f "airflow scheduler" || true
        pkill -f "airflow triggerer" || true

        sleep 3

        nohup /home/airflow/.local/bin/airflow scheduler > ~/airflow-scheduler.log 2>&1 &
        nohup /home/airflow/.local/bin/airflow webserver > ~/airflow-webserver.log 2>&1 &
        nohup /home/airflow/.local/bin/airflow triggerer > ~/airflow-triggerer.log 2>&1 &

        echo "âœ… Airflow Main ë°°í¬ ì™„ë£Œ"


  # Self-hosted runnerë¡œ Worker ì„œë²„ ë°°í¬
  deploy-to-worker:
    needs: deploy-to-main
    runs-on: [self-hosted, airflow-worker]

    steps:
    
    - name: Clean pycache
      run: |
        sudo find /home/ubuntu/airflow/dags -type d -name "__pycache__" -exec rm -rf {} +

    - name: Sync DAGs from S3
      run: |
        echo "ğŸš€ Airflow Worker ë°°í¬ ì‹œì‘..."
        
        # S3ì—ì„œ DAGs ë™ê¸°í™”
        aws s3 sync s3://${{ vars.S3_BUCKET_NAME }}/airflow/dags/ \
          /home/ubuntu/airflow/dags/ \
        --delete \
        --exclude "__pycache__/*" \
        --exclude "**/__pycache__/*" \
        --exclude "*.pyc"
      
        echo "âœ… DAGs ë™ê¸°í™” ì™„ë£Œ"
    
    - name: Restart Airflow (manual)
      run: |
        echo "ğŸ”„ Airflow processes ì¬ì‹œì‘ì¤‘..."

        pkill -f "airflow webserver" || true
        pkill -f "airflow scheduler" || true
        pkill -f "airflow triggerer" || true

        sleep 3

        nohup /home/airflow/.local/bin/airflow scheduler > ~/airflow-scheduler.log 2>&1 &
        nohup /home/airflow/.local/bin/airflow webserver > ~/airflow-webserver.log 2>&1 &
        nohup /home/airflow/.local/bin/airflow triggerer > ~/airflow-triggerer.log 2>&1 &

        echo "âœ… Airflow ë°°í¬ ì™„ë£Œ"


  # ë°°í¬ ì™„ë£Œ ì•Œë¦¼
  notify-deployment:
    needs: [deploy-to-main, deploy-to-worker]
    if: success()
    runs-on: ubuntu-latest
    
    steps:
    - name: Deployment Success
      run: |
        echo "ğŸ‰ ë°°í¬ ì™„ë£Œ!"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo "ğŸ“¦ S3 Bucket: ${{ vars.S3_BUCKET_NAME }}"
        echo "ğŸ–¥ï¸  Airflow Main: team7-airflow-main"
        echo "âš™ï¸  Airflow Worker: team7-airflow-worker"
        echo "ğŸ• Deployed at: $(date '+%Y-%m-%d %H:%M:%S')"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
